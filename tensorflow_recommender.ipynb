{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow and Deep Learning Malaysia Meetup Sept 2018\n",
    "\n",
    "Recommender System using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Importing tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing some more libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# reading the ratings data\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep=\"::\", header = None, engine='python')\n",
    "\n",
    "# Lets pivot the data to get it at a user level\n",
    "ratings_pivot = pd.pivot_table(ratings[[0,1,2]], values=2, index=0, columns=1 ).fillna(0)\n",
    "\n",
    "# creating train and test sets\n",
    "X_train, X_test = train_test_split(ratings_pivot, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deciding how many nodes wach layer should have\n",
    "n_nodes_inpl = 3706  \n",
    "n_nodes_hl1  = 256  \n",
    "n_nodes_outl = 3706\n",
    "\n",
    "# first hidden layer has 784*32 weights and 32 biases\n",
    "hidden_1_layer_vals = {'weights':tf.Variable(tf.random_normal ([n_nodes_inpl+1,n_nodes_hl1]))}\n",
    "\n",
    "# first hidden layer has 784*32 weights and 32 biases\n",
    "output_layer_vals = {'weights':tf.Variable(tf.random_normal ([n_nodes_hl1+1,n_nodes_outl])) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user with 3706 ratings goes in\n",
    "input_layer = tf.placeholder('float', [None, 3706])\n",
    "\n",
    "# add a constant node to the first layer\n",
    "# it needs to have the same shape as the input layer for me to be\n",
    "# able to concatinate it later\n",
    "\n",
    "input_layer_const = tf.fill( [tf.shape(input_layer)[0], 1] ,1.0  )\n",
    "input_layer_concat =  tf.concat([input_layer, input_layer_const], 1)\n",
    "\n",
    "# multiply output of input_layer wth a weight matrix \n",
    "layer_1 = tf.nn.sigmoid(tf.matmul(input_layer_concat, hidden_1_layer_vals['weights']))\n",
    "\n",
    "# adding one bias node to the hidden layer\n",
    "layer1_const = tf.fill( [tf.shape(layer_1)[0], 1] ,1.0  )\n",
    "layer_concat =  tf.concat([layer_1, layer1_const], 1)\n",
    "\n",
    "# multiply output of hidden with a weight matrix to get final output\n",
    "output_layer = tf.matmul( layer_concat,output_layer_vals['weights'])\n",
    "\n",
    "# output_true shall have the original shape for error calculations\n",
    "output_true = tf.placeholder('float', [None, 3706])\n",
    "\n",
    "# define our cost function\n",
    "meansq =    tf.reduce_mean(tf.square(output_layer - output_true))\n",
    "# define our optimizer\n",
    "\n",
    "learn_rate = 0.1   # how fast the model should learn\n",
    "optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising variables and starting the session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# defining batch size, number of epochs and learning rate\n",
    "batch_size = 100  # how many images to use together for training\n",
    "hm_epochs =200    # how many times to go through the entire dataset\n",
    "tot_users = X_train.shape[0] # total number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train 59.18676900600182 MSE test 58.46639464491769\n",
      "Epoch 0 / 200 loss: 4044.6285552978516\n",
      "MSE train 39.64310267008742 MSE test 38.72505068099756\n",
      "Epoch 1 / 200 loss: 2331.7318801879883\n",
      "MSE train 28.835596204435262 MSE test 28.040733640283474\n",
      "Epoch 2 / 200 loss: 1632.8321571350098\n",
      "MSE train 22.292538173099565 MSE test 21.667092933946513\n",
      "Epoch 3 / 200 loss: 1225.2616271972656\n",
      "MSE train 18.226457598807546 MSE test 17.751587227061087\n",
      "Epoch 4 / 200 loss: 973.7096853256226\n",
      "MSE train 15.527897946049057 MSE test 15.163832910418272\n",
      "Epoch 5 / 200 loss: 813.8804092407227\n",
      "MSE train 13.61424250537136 MSE test 13.377177554261115\n",
      "Epoch 6 / 200 loss: 703.4661560058594\n",
      "MSE train 12.205239096944625 MSE test 12.030698148975208\n",
      "Epoch 7 / 200 loss: 624.1632089614868\n",
      "MSE train 11.09787605330402 MSE test 10.9911187586128\n",
      "Epoch 8 / 200 loss: 563.1522836685181\n",
      "MSE train 10.204412749818175 MSE test 10.140432278631266\n",
      "Epoch 9 / 200 loss: 514.9302706718445\n",
      "MSE train 9.487894121686365 MSE test 9.475312792632826\n",
      "Epoch 10 / 200 loss: 476.0920991897583\n",
      "MSE train 8.867033339292451 MSE test 8.916323111511398\n",
      "Epoch 11 / 200 loss: 443.75744676589966\n",
      "MSE train 8.337115765286796 MSE test 8.439132047762975\n",
      "Epoch 12 / 200 loss: 415.8925166130066\n",
      "MSE train 7.8924767737242565 MSE test 8.03873059851543\n",
      "Epoch 13 / 200 loss: 392.046835899353\n",
      "MSE train 7.508384413919758 MSE test 7.695050650801939\n",
      "Epoch 14 / 200 loss: 371.96632385253906\n",
      "MSE train 7.176365534994463 MSE test 7.396763380712517\n",
      "Epoch 15 / 200 loss: 354.5391917228699\n",
      "MSE train 6.8901165957952815 MSE test 7.136984520817593\n",
      "Epoch 16 / 200 loss: 339.46947717666626\n",
      "MSE train 6.631116827597544 MSE test 6.905673024441028\n",
      "Epoch 17 / 200 loss: 326.2776827812195\n",
      "MSE train 6.401011194855437 MSE test 6.694483809907918\n",
      "Epoch 18 / 200 loss: 314.334659576416\n",
      "MSE train 6.189181967077936 MSE test 6.499113110513058\n",
      "Epoch 19 / 200 loss: 303.63832235336304\n",
      "MSE train 5.99457737428628 MSE test 6.315780348586694\n",
      "Epoch 20 / 200 loss: 293.75753927230835\n",
      "MSE train 5.821156344292719 MSE test 6.154056676409589\n",
      "Epoch 21 / 200 loss: 284.79469108581543\n",
      "MSE train 5.658279739319114 MSE test 6.004979368859982\n",
      "Epoch 22 / 200 loss: 276.65561628341675\n",
      "MSE train 5.5067494184571 MSE test 5.8660411191448185\n",
      "Epoch 23 / 200 loss: 269.0041358470917\n",
      "MSE train 5.370527553471743 MSE test 5.737842050566512\n",
      "Epoch 24 / 200 loss: 261.96525621414185\n",
      "MSE train 5.243910107775696 MSE test 5.61719953037405\n",
      "Epoch 25 / 200 loss: 255.5974428653717\n",
      "MSE train 5.12589485377118 MSE test 5.502427109663228\n",
      "Epoch 26 / 200 loss: 249.63856482505798\n",
      "MSE train 5.015801001698452 MSE test 5.393571284491374\n",
      "Epoch 27 / 200 loss: 244.09005284309387\n",
      "MSE train 4.912463241477923 MSE test 5.290627261696926\n",
      "Epoch 28 / 200 loss: 238.90002012252808\n",
      "MSE train 4.815827452482588 MSE test 5.193222013944382\n",
      "Epoch 29 / 200 loss: 234.03692317008972\n",
      "MSE train 4.7252577243969895 MSE test 5.101709569335558\n",
      "Epoch 30 / 200 loss: 229.4886109828949\n",
      "MSE train 4.6388299507665165 MSE test 5.0144012878418325\n",
      "Epoch 31 / 200 loss: 225.1939353942871\n",
      "MSE train 4.555643167249146 MSE test 4.931846342251334\n",
      "Epoch 32 / 200 loss: 221.09296035766602\n",
      "MSE train 4.478468974344169 MSE test 4.855427351618079\n",
      "Epoch 33 / 200 loss: 217.17724299430847\n",
      "MSE train 4.404349554194554 MSE test 4.782937373892412\n",
      "Epoch 34 / 200 loss: 213.5359878540039\n",
      "MSE train 4.332018947165732 MSE test 4.712566423555155\n",
      "Epoch 35 / 200 loss: 210.00660014152527\n",
      "MSE train 4.262655482676703 MSE test 4.645401492137768\n",
      "Epoch 36 / 200 loss: 206.5718994140625\n",
      "MSE train 4.197765216672592 MSE test 4.5819212371125815\n",
      "Epoch 37 / 200 loss: 203.29698705673218\n",
      "MSE train 4.136301451411841 MSE test 4.52127833396689\n",
      "Epoch 38 / 200 loss: 200.22428798675537\n",
      "MSE train 4.0777750266183 MSE test 4.463427365859622\n",
      "Epoch 39 / 200 loss: 197.30937886238098\n",
      "MSE train 4.021806842902059 MSE test 4.4077491104122695\n",
      "Epoch 40 / 200 loss: 194.53133416175842\n",
      "MSE train 3.9680829490878 MSE test 4.353862944355114\n",
      "Epoch 41 / 200 loss: 191.86677932739258\n",
      "MSE train 3.9166972761750527 MSE test 4.302071524421245\n",
      "Epoch 42 / 200 loss: 189.31046223640442\n",
      "MSE train 3.866400250771287 MSE test 4.251556619740442\n",
      "Epoch 43 / 200 loss: 186.86171340942383\n",
      "MSE train 3.816450192133337 MSE test 4.2022898364507\n",
      "Epoch 44 / 200 loss: 184.44930243492126\n",
      "MSE train 3.7687262009731115 MSE test 4.155423182550141\n",
      "Epoch 45 / 200 loss: 182.07778644561768\n",
      "MSE train 3.7222427090344805 MSE test 4.1099708475279355\n",
      "Epoch 46 / 200 loss: 179.80336833000183\n",
      "MSE train 3.676140053236918 MSE test 4.065568686609617\n",
      "Epoch 47 / 200 loss: 177.5809850692749\n",
      "MSE train 3.631968941311113 MSE test 4.02252312242571\n",
      "Epoch 48 / 200 loss: 175.38895726203918\n",
      "MSE train 3.5903554304591387 MSE test 3.981106482730707\n",
      "Epoch 49 / 200 loss: 173.2944474220276\n",
      "MSE train 3.55069413237704 MSE test 3.9416073369970115\n",
      "Epoch 50 / 200 loss: 171.3226969242096\n",
      "MSE train 3.512501195177403 MSE test 3.903044842694624\n",
      "Epoch 51 / 200 loss: 169.4342966079712\n",
      "MSE train 3.475217813285496 MSE test 3.865584234108619\n",
      "Epoch 52 / 200 loss: 167.6145260334015\n",
      "MSE train 3.4378354044927133 MSE test 3.8286439570783832\n",
      "Epoch 53 / 200 loss: 165.83014369010925\n",
      "MSE train 3.4015972381484776 MSE test 3.7926282219109955\n",
      "Epoch 54 / 200 loss: 164.05116391181946\n",
      "MSE train 3.3662496543199216 MSE test 3.757738329874248\n",
      "Epoch 55 / 200 loss: 162.32915329933167\n",
      "MSE train 3.3318929366449637 MSE test 3.7240835581249665\n",
      "Epoch 56 / 200 loss: 160.6458055973053\n",
      "MSE train 3.298799108051003 MSE test 3.6914816551281127\n",
      "Epoch 57 / 200 loss: 159.01112627983093\n",
      "MSE train 3.2662731994206693 MSE test 3.6595100105047287\n",
      "Epoch 58 / 200 loss: 157.4287452697754\n",
      "MSE train 3.234984238844601 MSE test 3.6288391392111152\n",
      "Epoch 59 / 200 loss: 155.88208627700806\n",
      "MSE train 3.2050426321879217 MSE test 3.59950517818412\n",
      "Epoch 60 / 200 loss: 154.39484691619873\n",
      "MSE train 3.1760896922879387 MSE test 3.5711968252357056\n",
      "Epoch 61 / 200 loss: 152.96970200538635\n",
      "MSE train 3.148052812614555 MSE test 3.5436741340369893\n",
      "Epoch 62 / 200 loss: 151.59056091308594\n",
      "MSE train 3.1207984104001127 MSE test 3.517153074475143\n",
      "Epoch 63 / 200 loss: 150.2529752254486\n",
      "MSE train 3.093601315997189 MSE test 3.491322981473533\n",
      "Epoch 64 / 200 loss: 148.94752264022827\n",
      "MSE train 3.0670299328223427 MSE test 3.466447077779352\n",
      "Epoch 65 / 200 loss: 147.64722990989685\n",
      "MSE train 3.0403482408960736 MSE test 3.4422516954857842\n",
      "Epoch 66 / 200 loss: 146.3735318183899\n",
      "MSE train 3.0137568860987005 MSE test 3.4186094086227157\n",
      "Epoch 67 / 200 loss: 145.09546160697937\n",
      "MSE train 2.9878491257664277 MSE test 3.395554113599905\n",
      "Epoch 68 / 200 loss: 143.822429895401\n",
      "MSE train 2.9625473382361163 MSE test 3.373001961836542\n",
      "Epoch 69 / 200 loss: 142.5889391899109\n",
      "MSE train 2.9378148286997177 MSE test 3.350922897501377\n",
      "Epoch 70 / 200 loss: 141.3808045387268\n",
      "MSE train 2.914164383977586 MSE test 3.32944215066642\n",
      "Epoch 71 / 200 loss: 140.20258915424347\n",
      "MSE train 2.8909533413084945 MSE test 3.308458743171206\n",
      "Epoch 72 / 200 loss: 139.07861518859863\n",
      "MSE train 2.868508356837509 MSE test 3.287939530482101\n",
      "Epoch 73 / 200 loss: 137.97397816181183\n",
      "MSE train 2.8468864267349643 MSE test 3.2679633658343\n",
      "Epoch 74 / 200 loss: 136.90471303462982\n",
      "MSE train 2.825887172141018 MSE test 3.248561066342179\n",
      "Epoch 75 / 200 loss: 135.87246429920197\n",
      "MSE train 2.805604326526732 MSE test 3.229712481886405\n",
      "Epoch 76 / 200 loss: 134.8729819059372\n",
      "MSE train 2.785962842087794 MSE test 3.211359287708729\n",
      "Epoch 77 / 200 loss: 133.90767669677734\n",
      "MSE train 2.766257023761433 MSE test 3.19331850009585\n",
      "Epoch 78 / 200 loss: 132.9681340456009\n",
      "MSE train 2.746883786041283 MSE test 3.1755154555910377\n",
      "Epoch 79 / 200 loss: 132.027423620224\n",
      "MSE train 2.7285415639386215 MSE test 3.1580825209853542\n",
      "Epoch 80 / 200 loss: 131.1085501909256\n",
      "MSE train 2.710456351699469 MSE test 3.140859654812736\n",
      "Epoch 81 / 200 loss: 130.23377680778503\n",
      "MSE train 2.6931270516405705 MSE test 3.1240683377181813\n",
      "Epoch 82 / 200 loss: 129.37360382080078\n",
      "MSE train 2.6760234362793334 MSE test 3.107547244656395\n",
      "Epoch 83 / 200 loss: 128.54678189754486\n",
      "MSE train 2.6594308730358427 MSE test 3.0913136324301225\n",
      "Epoch 84 / 200 loss: 127.73080241680145\n",
      "MSE train 2.642841333925503 MSE test 3.07524628863037\n",
      "Epoch 85 / 200 loss: 126.93691873550415\n",
      "MSE train 2.6263915532208064 MSE test 3.0593904478810803\n",
      "Epoch 86 / 200 loss: 126.14239013195038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train 2.6102872435415327 MSE test 3.0439035953495086\n",
      "Epoch 87 / 200 loss: 125.35692226886749\n",
      "MSE train 2.5944286570259822 MSE test 3.028730080896134\n",
      "Epoch 88 / 200 loss: 124.588019490242\n",
      "MSE train 2.5788204065554967 MSE test 3.013790323473512\n",
      "Epoch 89 / 200 loss: 123.82994854450226\n",
      "MSE train 2.5633340891852585 MSE test 2.9989738808352606\n",
      "Epoch 90 / 200 loss: 123.0861028432846\n",
      "MSE train 2.548246149124488 MSE test 2.9846091273870052\n",
      "Epoch 91 / 200 loss: 122.34780442714691\n",
      "MSE train 2.533250287992639 MSE test 2.970548958228801\n",
      "Epoch 92 / 200 loss: 121.6274664402008\n",
      "MSE train 2.518249577085702 MSE test 2.9565524260822063\n",
      "Epoch 93 / 200 loss: 120.91036474704742\n",
      "MSE train 2.503597547730121 MSE test 2.9428865688562844\n",
      "Epoch 94 / 200 loss: 120.19228446483612\n",
      "MSE train 2.4894798684824275 MSE test 2.929570168238711\n",
      "Epoch 95 / 200 loss: 119.49170923233032\n",
      "MSE train 2.4761274688402968 MSE test 2.916762909297479\n",
      "Epoch 96 / 200 loss: 118.82160651683807\n",
      "MSE train 2.4627603635120425 MSE test 2.9042419147251426\n",
      "Epoch 97 / 200 loss: 118.18430769443512\n",
      "MSE train 2.4493646828882985 MSE test 2.8918858426833736\n",
      "Epoch 98 / 200 loss: 117.54503619670868\n",
      "MSE train 2.4362259063879197 MSE test 2.8798215236559446\n",
      "Epoch 99 / 200 loss: 116.90622270107269\n",
      "MSE train 2.423274187133179 MSE test 2.8678925955267043\n",
      "Epoch 100 / 200 loss: 116.27789616584778\n",
      "MSE train 2.410289694516498 MSE test 2.8560852703450634\n",
      "Epoch 101 / 200 loss: 115.65740084648132\n",
      "MSE train 2.3971118434157566 MSE test 2.8443008611841143\n",
      "Epoch 102 / 200 loss: 115.0348207950592\n",
      "MSE train 2.384045998806677 MSE test 2.832853805991136\n",
      "Epoch 103 / 200 loss: 114.40633690357208\n",
      "MSE train 2.371346552387179 MSE test 2.8215871890781687\n",
      "Epoch 104 / 200 loss: 113.7829087972641\n",
      "MSE train 2.359362620002512 MSE test 2.810513244417997\n",
      "Epoch 105 / 200 loss: 113.17858743667603\n",
      "MSE train 2.34788491222489 MSE test 2.7996156386311735\n",
      "Epoch 106 / 200 loss: 112.60768365859985\n",
      "MSE train 2.336569936878346 MSE test 2.7888080701284323\n",
      "Epoch 107 / 200 loss: 112.0610077381134\n",
      "MSE train 2.3253667865671535 MSE test 2.778028940781183\n",
      "Epoch 108 / 200 loss: 111.52173018455505\n",
      "MSE train 2.3144077116508006 MSE test 2.7673456253232107\n",
      "Epoch 109 / 200 loss: 110.98671472072601\n",
      "MSE train 2.303511450990233 MSE test 2.7567066775797175\n",
      "Epoch 110 / 200 loss: 110.46289706230164\n",
      "MSE train 2.292788666143627 MSE test 2.746216825866533\n",
      "Epoch 111 / 200 loss: 109.94281375408173\n",
      "MSE train 2.2820643731271453 MSE test 2.7357955848220445\n",
      "Epoch 112 / 200 loss: 109.42970967292786\n",
      "MSE train 2.271569993609265 MSE test 2.7256848972912757\n",
      "Epoch 113 / 200 loss: 108.9187490940094\n",
      "MSE train 2.261353158650697 MSE test 2.7158471502288726\n",
      "Epoch 114 / 200 loss: 108.41810488700867\n",
      "MSE train 2.251362754961056 MSE test 2.706255356198147\n",
      "Epoch 115 / 200 loss: 107.93081653118134\n",
      "MSE train 2.2414978620316712 MSE test 2.6969319869828867\n",
      "Epoch 116 / 200 loss: 107.45400834083557\n",
      "MSE train 2.2315633669788504 MSE test 2.687715465166851\n",
      "Epoch 117 / 200 loss: 106.98299825191498\n",
      "MSE train 2.221712523061442 MSE test 2.6786417885040916\n",
      "Epoch 118 / 200 loss: 106.50808715820312\n",
      "MSE train 2.2121185854615577 MSE test 2.6697968455048295\n",
      "Epoch 119 / 200 loss: 106.03822839260101\n",
      "MSE train 2.2026340065971723 MSE test 2.661135716020019\n",
      "Epoch 120 / 200 loss: 105.58095872402191\n",
      "MSE train 2.193291505473209 MSE test 2.652661406488181\n",
      "Epoch 121 / 200 loss: 105.1285890340805\n",
      "MSE train 2.1839673557078187 MSE test 2.64432181473742\n",
      "Epoch 122 / 200 loss: 104.68343377113342\n",
      "MSE train 2.1748595769726498 MSE test 2.6360759689225364\n",
      "Epoch 123 / 200 loss: 104.23916018009186\n",
      "MSE train 2.1658535747737653 MSE test 2.627910594634028\n",
      "Epoch 124 / 200 loss: 103.80448698997498\n",
      "MSE train 2.156886214141362 MSE test 2.619772518071088\n",
      "Epoch 125 / 200 loss: 103.37363481521606\n",
      "MSE train 2.1478196419313034 MSE test 2.6117368939543417\n",
      "Epoch 126 / 200 loss: 102.94442629814148\n",
      "MSE train 2.1388673085290897 MSE test 2.60386402288495\n",
      "Epoch 127 / 200 loss: 102.51205968856812\n",
      "MSE train 2.1300835861220593 MSE test 2.596211450959462\n",
      "Epoch 128 / 200 loss: 102.08343696594238\n",
      "MSE train 2.1215890278665177 MSE test 2.5887115447620466\n",
      "Epoch 129 / 200 loss: 101.66464233398438\n",
      "MSE train 2.113611354141338 MSE test 2.5814061565021973\n",
      "Epoch 130 / 200 loss: 101.2612167596817\n",
      "MSE train 2.1058448960414093 MSE test 2.5742210728870605\n",
      "Epoch 131 / 200 loss: 100.88023328781128\n",
      "MSE train 2.0983219934578634 MSE test 2.567106607241498\n",
      "Epoch 132 / 200 loss: 100.50974035263062\n",
      "MSE train 2.0909005181939926 MSE test 2.5601641932821417\n",
      "Epoch 133 / 200 loss: 100.15185618400574\n",
      "MSE train 2.083522987632155 MSE test 2.5533288087981063\n",
      "Epoch 134 / 200 loss: 99.79768395423889\n",
      "MSE train 2.0760753226644546 MSE test 2.546583709735637\n",
      "Epoch 135 / 200 loss: 99.44676458835602\n",
      "MSE train 2.0688428895105777 MSE test 2.539950998943487\n",
      "Epoch 136 / 200 loss: 99.09147202968597\n",
      "MSE train 2.061890686783227 MSE test 2.5335102161119156\n",
      "Epoch 137 / 200 loss: 98.74762749671936\n",
      "MSE train 2.055273853545764 MSE test 2.5272455758902925\n",
      "Epoch 138 / 200 loss: 98.41714119911194\n",
      "MSE train 2.0486735379544676 MSE test 2.521100406553128\n",
      "Epoch 139 / 200 loss: 98.10093677043915\n",
      "MSE train 2.0419408686474716 MSE test 2.515011969927613\n",
      "Epoch 140 / 200 loss: 97.78558349609375\n",
      "MSE train 2.03513816824411 MSE test 2.5089576219139356\n",
      "Epoch 141 / 200 loss: 97.4629340171814\n",
      "MSE train 2.0283662715311945 MSE test 2.503050184268082\n",
      "Epoch 142 / 200 loss: 97.13661742210388\n",
      "MSE train 2.021680539350576 MSE test 2.497388298605347\n",
      "Epoch 143 / 200 loss: 96.81342113018036\n",
      "MSE train 2.0151344492500267 MSE test 2.4918266159436193\n",
      "Epoch 144 / 200 loss: 96.49406266212463\n",
      "MSE train 2.008734204183137 MSE test 2.4863448926681233\n",
      "Epoch 145 / 200 loss: 96.18229675292969\n",
      "MSE train 2.0024242830389234 MSE test 2.4809142709767924\n",
      "Epoch 146 / 200 loss: 95.87795770168304\n",
      "MSE train 1.9960560731077308 MSE test 2.475506790749646\n",
      "Epoch 147 / 200 loss: 95.57710933685303\n",
      "MSE train 1.9897192069859069 MSE test 2.470131809818291\n",
      "Epoch 148 / 200 loss: 95.27411103248596\n",
      "MSE train 1.9835640158471741 MSE test 2.4648616681205318\n",
      "Epoch 149 / 200 loss: 94.97195065021515\n",
      "MSE train 1.9774338899179102 MSE test 2.459701317930367\n",
      "Epoch 150 / 200 loss: 94.678102850914\n",
      "MSE train 1.9713709198527245 MSE test 2.4546005700993923\n",
      "Epoch 151 / 200 loss: 94.38567078113556\n",
      "MSE train 1.9654126166578592 MSE test 2.4495513811709677\n",
      "Epoch 152 / 200 loss: 94.09718370437622\n",
      "MSE train 1.9594191416404287 MSE test 2.444442285178826\n",
      "Epoch 153 / 200 loss: 93.81350421905518\n",
      "MSE train 1.9532508636726402 MSE test 2.43925617175441\n",
      "Epoch 154 / 200 loss: 93.52678215503693\n",
      "MSE train 1.947420893043154 MSE test 2.4342787437463493\n",
      "Epoch 155 / 200 loss: 93.23264968395233\n",
      "MSE train 1.941696877466473 MSE test 2.429439498960841\n",
      "Epoch 156 / 200 loss: 92.95452380180359\n",
      "MSE train 1.9359467181407983 MSE test 2.4246518426307526\n",
      "Epoch 157 / 200 loss: 92.68107306957245\n",
      "MSE train 1.9304195399678994 MSE test 2.4199298206318427\n",
      "Epoch 158 / 200 loss: 92.40667569637299\n",
      "MSE train 1.92516407079206 MSE test 2.415238628404061\n",
      "Epoch 159 / 200 loss: 92.14427995681763\n",
      "MSE train 1.9197959642507707 MSE test 2.410528926655323\n",
      "Epoch 160 / 200 loss: 91.89334511756897\n",
      "MSE train 1.9144328437024996 MSE test 2.405830092168482\n",
      "Epoch 161 / 200 loss: 91.63710916042328\n",
      "MSE train 1.9091226778261645 MSE test 2.401155393865667\n",
      "Epoch 162 / 200 loss: 91.38217902183533\n",
      "MSE train 1.9038761616521418 MSE test 2.396525166259165\n",
      "Epoch 163 / 200 loss: 91.1293693780899\n",
      "MSE train 1.8986368927037702 MSE test 2.391979814715529\n",
      "Epoch 164 / 200 loss: 90.87934505939484\n",
      "MSE train 1.8935631228546979 MSE test 2.387580322037253\n",
      "Epoch 165 / 200 loss: 90.6294447183609\n",
      "MSE train 1.8886203899449219 MSE test 2.383267824255934\n",
      "Epoch 166 / 200 loss: 90.38783645629883\n",
      "MSE train 1.8838181599937258 MSE test 2.3790074501697855\n",
      "Epoch 167 / 200 loss: 90.15183174610138\n",
      "MSE train 1.8791608777394948 MSE test 2.374792648799555\n",
      "Epoch 168 / 200 loss: 89.92350673675537\n",
      "MSE train 1.8744746692727121 MSE test 2.3706115581124028\n",
      "Epoch 169 / 200 loss: 89.70148301124573\n",
      "MSE train 1.8698295226434796 MSE test 2.3664907797282924\n",
      "Epoch 170 / 200 loss: 89.47789025306702\n",
      "MSE train 1.8652788860047838 MSE test 2.362423209742486\n",
      "Epoch 171 / 200 loss: 89.25639474391937\n",
      "MSE train 1.8607257211140278 MSE test 2.358377491527846\n",
      "Epoch 172 / 200 loss: 89.03911507129669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train 1.8559137447089462 MSE test 2.354310437591753\n",
      "Epoch 173 / 200 loss: 88.82133793830872\n",
      "MSE train 1.85094830611323 MSE test 2.3502549804720827\n",
      "Epoch 174 / 200 loss: 88.59145927429199\n",
      "MSE train 1.8464640563946424 MSE test 2.346331620443487\n",
      "Epoch 175 / 200 loss: 88.35449290275574\n",
      "MSE train 1.8422137291298843 MSE test 2.3425498337938064\n",
      "Epoch 176 / 200 loss: 88.1411645412445\n",
      "MSE train 1.8380688441771826 MSE test 2.338803189231594\n",
      "Epoch 177 / 200 loss: 87.93859851360321\n",
      "MSE train 1.8339293736893925 MSE test 2.33505300078013\n",
      "Epoch 178 / 200 loss: 87.74081194400787\n",
      "MSE train 1.8297901519299022 MSE test 2.3312727333551844\n",
      "Epoch 179 / 200 loss: 87.54355275630951\n",
      "MSE train 1.8256279744575743 MSE test 2.327493264812756\n",
      "Epoch 180 / 200 loss: 87.34578227996826\n",
      "MSE train 1.8213031860222473 MSE test 2.323734351425725\n",
      "Epoch 181 / 200 loss: 87.14727437496185\n",
      "MSE train 1.8168282737141137 MSE test 2.3199806467042796\n",
      "Epoch 182 / 200 loss: 86.94065701961517\n",
      "MSE train 1.812406548651541 MSE test 2.3162841817535424\n",
      "Epoch 183 / 200 loss: 86.72688376903534\n",
      "MSE train 1.8081045439703072 MSE test 2.3126720543655312\n",
      "Epoch 184 / 200 loss: 86.51624834537506\n",
      "MSE train 1.8039781412884934 MSE test 2.309094037458642\n",
      "Epoch 185 / 200 loss: 86.31071829795837\n",
      "MSE train 1.7999076203992215 MSE test 2.3055024486723577\n",
      "Epoch 186 / 200 loss: 86.11341488361359\n",
      "MSE train 1.795774890036615 MSE test 2.301879168895274\n",
      "Epoch 187 / 200 loss: 85.91881167888641\n",
      "MSE train 1.7915671771791954 MSE test 2.2982320089303303\n",
      "Epoch 188 / 200 loss: 85.72132122516632\n",
      "MSE train 1.7874140723598384 MSE test 2.2946066133989667\n",
      "Epoch 189 / 200 loss: 85.52031564712524\n",
      "MSE train 1.7834327357968722 MSE test 2.291128647740909\n",
      "Epoch 190 / 200 loss: 85.3222804069519\n",
      "MSE train 1.779526056312695 MSE test 2.2877878360915136\n",
      "Epoch 191 / 200 loss: 85.13211643695831\n",
      "MSE train 1.775614841355824 MSE test 2.284530860788022\n",
      "Epoch 192 / 200 loss: 84.94608497619629\n",
      "MSE train 1.7718297987755116 MSE test 2.2813576554643675\n",
      "Epoch 193 / 200 loss: 84.76034116744995\n",
      "MSE train 1.76816749056087 MSE test 2.278224530060315\n",
      "Epoch 194 / 200 loss: 84.58026671409607\n",
      "MSE train 1.7644391779749355 MSE test 2.275092350376535\n",
      "Epoch 195 / 200 loss: 84.40512251853943\n",
      "MSE train 1.7606187600852856 MSE test 2.2719604890170104\n",
      "Epoch 196 / 200 loss: 84.2268557548523\n",
      "MSE train 1.7569708357759435 MSE test 2.2688788299610505\n",
      "Epoch 197 / 200 loss: 84.04500997066498\n",
      "MSE train 1.753627742058626 MSE test 2.265918736986785\n",
      "Epoch 198 / 200 loss: 83.87119472026825\n",
      "MSE train 1.750426882644464 MSE test 2.2630605798638155\n",
      "Epoch 199 / 200 loss: 83.71232414245605\n"
     ]
    }
   ],
   "source": [
    "# running the model for a 200 epochs taking 100 users in batches\n",
    "# total improvement is printed out after each epoch\n",
    "\n",
    "for epoch in range(hm_epochs):\n",
    "    epoch_loss = 0    # initializing error as 0\n",
    "    \n",
    "    for i in range(int(tot_users/batch_size)):\n",
    "        epoch_x = X_train[ i*batch_size : (i+1)*batch_size ]\n",
    "        _, c = sess.run([optimizer, meansq],\\\n",
    "               feed_dict={input_layer: epoch_x, \\\n",
    "               output_true: epoch_x})\n",
    "        epoch_loss += c\n",
    "        \n",
    "    output_train = sess.run(output_layer, feed_dict={input_layer:X_train})\n",
    "    output_test = sess.run(output_layer, feed_dict={input_layer:X_test})\n",
    "        \n",
    "    print('MSE train', MSE(output_train, X_train),'MSE test', MSE(output_test, X_test))      \n",
    "    print('Epoch', epoch, '/', hm_epochs, 'loss:',epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a user\n",
    "sample_user = X_test.iloc[99,:]\n",
    "#get the predicted ratings\n",
    "sample_user_pred = sess.run(output_layer, feed_dict={input_layer:[sample_user]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.461074  , -0.478554  ,  2.5884945 , ..., -0.4671824 ,\n",
       "         1.9086571 , -0.29935086]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_user_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
